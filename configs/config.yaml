model:
  model_type: codi # "codi" or "base"
  model_name_or_path: "EleutherAI/gpt-neo-125m"
  cot_length: 6
  max_length: 256
  use_lora: false
  lora:
    r: 128
    lora_alpha: 32
    target_modules: ["q_proj", "k_proj", "v_proj", "out_proj", "fc_in", "fc_out"]
    lora_dropout: 0.1
    bias: "none"
  # Special tokens
  bot_token: "<bot>"
  eot_token: "<eot>"
  device: "cuda:0"
  alpha: 1
  beta: 1
  gamma: 1
train:
  effective_batch_size: 16
  num_epochs: 2
  max_length: 256
  save_checkpoints_every: 10000 # num of batches
  save_checkpoints_dir: "./checkpoints"
  resume_from: "" # "./checkpoints/checkpoint_10000.pt" 
  validate_every: 1000 # num of batches
  warmup_ratio: 0.03
  optimizer:
    lr: 1e-4

data:
  dataset:
    name: "zen-E/GSM8k-Aug"
    split_train: "train"
    split_test: "test"
  dataloader:
    batch_size: 16 # Reduced for now
    train_shuffle: true
    test_shuffle: false
    num_workers: 0 # Reduced for now